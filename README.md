# Kaggle Competition - Titanic: Machine Learning from Disaster

This project is an entry in to the Kaggle competition 'Titanic: Machine Learning from Disaster' by August Semrau Andersen.

The scripts contained here are; dataLoader.py loads .csv data, models.py contains non-tuned classification models, predictions.py is used for printing predictions to .csv format for entry in kaggle-competition.



Below is a short description of methods used and which accuracy their use yielded.

- Logistic Regression, no tuning: 0.737 accuracy.


- Naive Bayes, no tuning: 0.718 accuracy.


- Stochastic Gradient Descent, squared_loss: 0.373 accuracy.
Obviously this approach underperforms, SDG is usually good with large amounts of data, which is not the case here.



- XGBoost : 0.775 accuracy.


Final rank on Kaggle:
